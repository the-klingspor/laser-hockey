\section{Rainbow}

The Rainbow algorithm~\cite{Hessel2018:Rainbow} is a collection of improvements upon the seminal Deep Q-Network (DQN) algorithm~\cite{mnih2015humanlevel} that revolutionized the use of deep learning architectures for reinforcement learning problems. \ref{subsec:rainbow_methods} introduces all techniques used in Rainbow, and \ref{subsec:rainbow_experimental} gives a detailed evaluation of the effect of individual components and further modifications made to the learning setup.

\subsection{Methods}\label{subsec:rainbow_methods}

Deep reinforcement learning is, as of 2023, the dominant approach to reinforcement learning problems. This paradigm introduces the use of deep neural networks for function approximation, giving powerful representations for policies $\pi_\theta(s, a)$ or values $\{\hat{v}(s, w)$, $\hat{q}(s, a, w)\}$ or both when the state space or the action space is intractably large or continuous.

In DQN, only the Q-value function is explicitly represented as $\hat{q}(s, a; w)$ and we aim to solve the Bellman optimality equation by minimizing the loss $\mathcal{L}(w) = \mathbb{E}_{s, a, r, s', d \sim \mathcal{D}}\left[\text{TD}(s, a, r, s', d)^2\right]$ where $\mathcal{D}$ represents a uniform replay buffer, and the tuple $(s, a, r, s', d)$ is collected and stored in $\mathcal{D}$ by an arbitrary off-policy agent (e.g., $\epsilon$-greedy w.r.t. $\hat{q}(\cdot, \cdot; w)$). The TD error is defined as $\text{TD}(s, a, r, s', d) = r + d\gamma \max_{a'} \hat{q}\left(s', a'; w^\text{target}\right) - \hat{q}(s, a; w)$ with $d = [\![ s' \text{ is terminal} ]\!]$. Using the notation introduced in the lecture, $w^\text{target}$ refers to the target network weights that are updated either by setting $w^\text{target} \gets w$ every $k$ gradient steps or by using an exponential moving average (the Polyak update).

Let us introduce the modifications made to the Markov Decision Process (MDP) to facilitate learning and make the agents more performant.

\subsubsection{Modifications to the Markov Decision Process}\label{subsubsec:mdp}

First, we consider a new reward discussed in~\ref{subsubsec:state_space} that leverages all parts of the information dictionary and leads to much faster convergence (\ref{subsubsec:base}). We also augment the observation space with pairwise distances between the opponents, the puck, and the goals, making the state space 27-dimensional. We further experiment with observation stacking where one state in the MDP contains $n$ consecutive observations/frames. Finally, we compare a basic 8-dimensional action space with an advanced 24-dimensional one, both listed in Appendix~\ref{appendix:rainbow}. Let us discuss the Rainbow algorithm's extensions compared to the base DQN algorithm.

\subsubsection{Prioritized Experience Replay}

We first consider the Prioritized Experience Replay (PER) extension~\cite{schaul2016prioritized} as it showed a consistent advantage over the regular experience replay using uniform sampling during our preliminary experiments. To sample transitions the agent can learn a lot from, PER samples them proportionally to their loss value: $P((s, a, r, s', n)) \propto \left(\begin{cases}0.5 \cdot \text{TD}(s, a, r, s', d)^2 & \text{if } |\text{TD}(s, a, r, s', d)| < 1 \\ |\text{TD}(s, a, r, s', d)| - 0.5 & \text{otherwise}\end{cases}\right)^{\alpha}$ where $\alpha$ is a hyperparameter we set to $0.5$.

\subsubsection{Dueling Networks}

We consider the Dueling Networks extension~\cite{wang2016:DDQN} next. Our initial experiments showed it to be the most important addition for the \texttt{laser-hockey} environment (\ref{subsec:rainbow_experimental}). In Dueling Networks, we separate the Q-value computation into an advantage and a value stream. These share a common feature extractor $f(s; \xi)$ that is used to calculate the advantage and value streams as $\hat{a}(f(s; \xi), a; \psi)$ and $\hat{v}(f(s; \xi); \eta)$. Finally, the Q-value is given by $\hat{q}(s, a; w) = \hat{v}(f(s; \xi); \eta) + \hat{a}(f(s; \xi), a; \psi) - \frac{1}{|\mathcal{A}|}\sum_{a'} \hat{a}(f(s; \xi), a'; \psi)$ where $\mathcal{A}$ is the discrete set of actions and $w = \{\xi, \eta, \psi\}$.

\subsubsection{Double Q-Learning with Multi-Step Returns}\label{subsubsec:double}

As seen in the lecture, there is an inherent overestimation bias in Q-Learning due to the max operator in the TD target. In particular, as the Q-values are noisy during training, we also take the maximum over the added noise, which results in a systematic bias. Double Q-Learning~\cite{vanhasselt2015deep} mitigates this bias by separating the selection of the maximizing action using the online network $\hat{q}(\cdot, \cdot; w)$ and the prediction of the value corresponding to the maximizing action using the target network $\hat{q}(\cdot, \cdot; w^\text{target})$. %Thus, the TD error becomes $\text{TD}(s, a, r, s', d) = r + d\gamma\hat{q}\left(s', \arg\max_{a'}\hat{q}(s', a'; w); w^\text{target}\right) - \hat{q}(s, a; w)$.
Multi-step returns generalize the one-step TD target to bootstrapping over $n$ steps using the $n$-step return $r_t^{(n)} = \sum_{k=0}^{n-1} \gamma^kr_{t+k+1}$. Combined with Double Q-Learning, the TD error becomes $\text{TD}(s_t, a_t, r_{t+1}, s_{t+1}, d) = r_t^{(n)} + d\gamma^n\hat{q}\left(s_{t+n}, \arg\max_{a'}\hat{q}(s_{t+n}, a'; w); w^\text{target}\right) - \hat{q}(s_t, a_t; w)$.

\subsubsection{Noisy Networks}\label{subsubsec:noisy}

Noisy Networks~\cite{fortunato2019noisy} use noisy linear layers instead of regular linear layers, which are of the form $f(x) = (\mu^w + \sigma^w \odot \epsilon^w)x + (\mu^b + \sigma^b \odot \epsilon^b)$ for input $x \in \mathbb{R}^{\text{in}}$ where $\mu^w, \sigma^w \in \mathbb{R}^{\text{out} \times \text{in}}$ and $\mu^b, \sigma^b$ are learnable parameters and $\epsilon^w \in \mathbb{R}^{\text{out} \times \text{in}}, \epsilon^b \in \mathbb{R}^{\text{out}}$ are random variables with $\epsilon^\text{out} \sim \mathcal{N}(0, I) \in \mathbb{R}^{\text{out}}$, $\epsilon^\text{in} \sim \mathcal{N}(0, I) \in \mathbb{R}^{\text{in}}$, $(\epsilon^W)_{ij} = f(\epsilon^\text{out}_i)f(\epsilon^\text{in}_j)$, $(\epsilon^b)_j = f(\epsilon^\text{out}_j)$ where $f(x) = \text{sgn}(x)\sqrt{|x|}$.
Noisy Networks empirically lead to better exploration than $\epsilon$-greedy strategies when the agent has to perform many actions to collect the first non-zero reward in the MDP~\cite{Hessel2018:Rainbow}. They give efficient state-dependent exploration without using $\epsilon$-greedy action selection.

\subsubsection{Distributional Reinforcement Learning}\label{subsubsec:distributional}

Distributional Reinforcement Learning~\cite{bellemare17distributional} aims to tackle the problem of stochastic returns. In particular, for state $s_t$ and action $a_t$, it predicts a discrete distribution of values $\hat{p}(s_t, a_t; w)$ over atoms $z$ on a uniform support between $\text{return}_\text{min}$ and $\text{return}_\text{max}$ instead of a single value $\hat{q}(s_t, a_t; w)$. To train an agent with Distributional Reinforcement Learning, we use a variant of the Bellman optimality equation where, for the optimal policy $\pi^*$, $\hat{p}(s_t, a_t; w)$ (defined on atoms $z$) has to match $\hat{p}(s_{t+1}, \arg\max_{a'} \hat{q}(s_{t+1}, a'; w^\text{target}); w)$ (defined on atoms $r_{t+1} + \gamma z$) when the latter is projected onto atoms $z$ using linear interpolation. Here, $\hat{q}(s_t, a_t; w) = z^\top \hat{p}(s_t, a_t; w)$. The closeness of the two distributions is measured by the Kullback-Leibler divergence. As our values are highly stochastic in the \texttt{laser-hockey} environment when not considering a fixed opponent, we were excited to see how Distributional Reinforcement Learning performs in our setting.

\subsection{Experimental Evaluation}\label{subsec:rainbow_experimental}

\subsubsection{Base Experiments}\label{subsubsec:base}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/basic_experiments_rainbow.pdf}
    \caption{Fraction of games won for different MDP setups and algorithm variants against the weak and strong opponents, respectively. Lines correspond to the mean performance; shaded areas correspond to minimum and maximum performance per time step across three seeds. The experiments are described in detail in~\ref{subsec:rainbow_experimental}.}
    \label{fig:basic_experiments_rainbow}
\end{figure}


In our base experiments, we aimed to understand how the modifications to the MDP detailed in \ref{subsubsec:mdp} affect the learning process and what the effects of certain algorithmic changes are. \textbf{Experiment 1} uses DQN with the original reward $r = \texttt{sparse\_reward}$ $+\ \texttt{reward\_closeness\_to\_puck}$ and prioritized experience replay. \textbf{Experiment 2} adds the advanced reward from~\ref{subsubsec:base}. \textbf{Experiment 3} adds dueling~\cite{wang2016:DDQN} to the architecture. \textbf{Experiment 4} augments the state space according to \ref{subsubsec:state_space}, and \textbf{Experiment 5} uses a set of 24 discrete actions detailed in Appendix~\ref{appendix:rainbow}. Finally, \textbf{Experiment 6} uses observation stacking with four frames (\ref{subsubsec:mdp}). All experiments use the same hyperparameter setup deemed best for the tournament (Appendix~\ref{appendix:rainbow}) and are run on three different seeds.

The results are shown in Fig.~\ref{fig:basic_experiments_rainbow}. \textbf{Experiment 2} highlights that considering a well-designed reward that highly facilitates learning does still not provide a strong enough learning signal for the baseline DQN algorithm. Interestingly, switching to a dueling architecture already allows the agent to learn sensible strategies, highlighting the prominance of dueling for the \texttt{laser-hockey} environment. Training this variant for more steps might also result in perfect convergence. Augmenting the state space leads to a notable improvement in convergence speed, and adding a more granular action space further contributes to the algorithm's stability. Finally, observation stacking also improves convergence and allows the agent to learn much more subtle strategies for the tournament during self-play training.

\subsubsection{Advanced Experiments}\label{subsubsec:advanced}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/advanced_experiments_rainbow.pdf}
    \caption{Average fraction of games won for different Rainbow additions against unseen opponents from \textbf{Experiment 5-6} (left) and against weak and strong basic opponents (right). Lines correspond to the mean performance; shaded areas correspond to minimum and maximum performance per time step across three seeds. The experiments are described in detail in~\ref{subsec:rainbow_experimental}.}
    \label{fig:advanced_experiments_rainbow}
\end{figure}

In our advanced experiments, we started from the dueling DQN baseline with prioritized experience replay from \textbf{Experiment 6} and evaluated the effects of the additional Rainbow components discussed in~\ref{subsec:rainbow_methods}. As half of the base experiments already converged to nearly perfect scores against the weak and strong opponents in six million frames, we set up a more challenging benchmark of beating the agents of \textbf{Experiments 5 and 6} (meaning six agents in total because of using three seeds per experiment). To this end, we trained each agent against the weak and strong opponents until they reached a win rate of 90\% against the strong opponent, then started self-playing by adding a copy of the agent's current state to the set of opponents we train against every 100,000 frames. All trained agents use the hyperparameters in Appendix~\ref{appendix:rainbow} and dueling: the modifications discussed below are \emph{additions} to this baseline. However, unlike \textbf{Experiments 1-6}, the following experiments do not consider cumulative additions to ensure we can understand the effect of additional components independently. For example, \textbf{Experiment 8} does not contain the additions incorporated in \textbf{Experiment 7}.

Fig.~\ref{fig:advanced_experiments_rainbow} shows the average win percentage against the six unseen agents over the course of training and also against the weak and strong opponents for the sake of comparability to Fig.~\ref{fig:basic_experiments_rainbow}. \textbf{Experiment 7} considers the baseline from \textbf{Experiment 6} but with self-playing as described above. \textbf{Experiment 8} adds double Q-learning instead with 3-step returns (\ref{subsubsec:double}). \textbf{Experiment 9} incorporates noisy networks (\ref{subsubsec:noisy}), and \textbf{Experiment 10} uses Distributional Reinforcement Learning (\ref{subsubsec:distributional}). Finally, \textbf{Experiment 11} uses the full Rainbow algorithm. The results show that none of the further additions increase performance considerably using the same hyperparameter base and recommended values for the additional hyperparameters. Distributional dueling DQN with PER is on par with the baseline regarding both convergence speed and performance. This architecture can represent the stochasticity in the \texttt{laser-hockey} environment well, therefore it is clear why this addition resulted in a very slight improvement in convergence speed. However, it takes considerably more time to run this architecture for 12 million time steps, thus we decided against using it in the tournament where every second counts for fine-tuning the agent on the collected data. The third-best variant is noisy dueling DQN with PER (\textbf{Experiment 9}). As discussed in \ref{subsubsec:noisy}, noisy networks are particularly useful when the agent must perform many correct actions to collect the first non-zero reward, which is the case for, e.g., exploration-based Atari games, but the \texttt{laser-hockey} environment does not require such complex long-term planning. Double Q-learning made the agent more unstable and did not lead to faster convergence. A notable problem with multi-step returns is that they make the algorithm on-policy: the \emph{future} rewards collected according to the noisy policy are also incorporated into the TD target. We did not find this addition useful for the \texttt{laser-hockey} environment. Finally, the full Rainbow algorithm failed to converge using the suggested hyperparameters in~\cite{Hessel2018:Rainbow} for the individual components. It was also by far the slowest variant of DQN.

\subsubsection{Limitations}

The number of experiments conducted made it infeasible to find a hyperparameter setup for each experiment that works best. We aimed to find a unified hyperparameter base that is stable across experiments and is performant for the final version we used for the tournament: the dueling DQN with prioritized experience replay. However, one might find even better hyperparameter setups for the individual experiments in a much more large-scale experiment that could influence the obtained results. Apart from the learning rate, we stayed quite close to the hyperparameters suggested in~\cite{Hessel2018:Rainbow} for all experiments (Appendix~\ref{appendix:rainbow}).
