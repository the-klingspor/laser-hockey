\section{Soft Actor-Critic (SAC)}
 SAC algorithm is an entropy-based off-policy learning algorithm for continuous control problem. To understand SAC algorithm better, we must first dive into the \textit{maximum-entropy RL} framework.

\textbf{Maximum Entropy Reinforcement Learning:} In conventional RL, the agent's motivation is to maximize the cumulative sum of the expected reward \cite{Sutton1998}. Maximum Entropy RL augments this objective function incorporating a term encouraging exploration \cite{max_ent}. According to this framework an optimal policy is expected to maximize reward while being as stochastic as possible. $ \pi^*=\arg \max _\pi \mathcal{J}(\pi) =\arg \max _\pi \sum_t \mathbb{E}_{\left(\mathbf{s}_t, \mathbf{a}_t\right) \sim \rho_\pi}\left[r\left(\mathbf{s}_t, \mathbf{a}_t\right)+\alpha \mathcal{H}\left(\pi\left(\cdot \mid \mathbf{s}_t\right)\right)\right],$ where  $\rho_\pi$ represents the probability distribution the policy, and $\mathcal{H}$ represents the entropy of the stochastic policy $\pi$ given state $s_{t}$. The temperature parameter $\alpha$ scales the randomness of the agent. For the case of $\alpha=0$ the objective originated from the reward hypothesis is obtained. 

Maximum Entropy formulation of the objective function brings about some advantages. Encouraged exploration is helpful to cure the problem of sticking to sub-optimal policies. This is also good for agent's containment of equally eligible multiple different policies. 

\textbf{Soft Policy Iteration: } Soft Policy iteration indicates the process of alternating between policy evaluation and policy improvement during learning maximum entropy policies. It is the corresponding concept name of policy iteration in maximum entropy RL framework \cite{HaarnojaAbbeelLevine2018:SAC}.  

In policy evaluation phase, our aim is to compute the value of a soft policy according to the maximum entropy objective. Therefore, we need to compute the soft Q-value, $Q\!:\!\mathcal{S}\text{x}\mathcal{A}\!\rightarrow\!\mathbb{R}$, for a fixed policy. This can be done via starting with any Q function and iteratively applying a modified version of Bellman update operation $\mathcal{T}^\pi$: $ \mathcal{T}^\pi Q\left(\mathbf{s}_t, \mathbf{a}_t\right) \triangleq r\left(\mathbf{s}_t, \mathbf{a}_t\right)+\gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim p}\left[V\left(\mathbf{s}_{t+1}\right)\right] $ where $    V\left(\mathbf{s}_t\right)=\mathbb{E}_{\mathbf{a}_t \sim \pi}\left[Q\left(\mathbf{s}_t, \mathbf{a}_t\right)-\log \pi\left(\mathbf{a}_t \mid \mathbf{s}_t\right)\right].
\label{equation:soft_policy_bellman}$ In former equation $V\!:\!\mathcal{S}\!\rightarrow\!\mathbb{R}$ indicates the soft value function. The remaining step of the soft policy iteration is policy improvement. For policy improvement, the set of policies $\Pi$ is restricted to parameterized family of distributions. This is a way of ensuring tractability, necessary for gradient flow in the learning process. The policy is improved via an update towards the exponential of the new Q-function. The authors used Kullback-Leibler divergence for updating current policy: $\pi_{\text {new }}=\arg \min _{\pi^{\prime} \in \Pi} \mathrm{D}_{\mathrm{KL}}\left(\pi^{\prime}\left(\cdot \mid \mathbf{s}_t\right) \bigg| \bigg| \frac{\exp \left(Q^{\pi_{\text {old }}}\left(\mathbf{s}_t, \cdot\right)\right)}{Z^{\pi_{\text {old }}}\left(\mathbf{s}_t\right)}\right).
$\label{eqn:soft_policy_kl} The partition function $Z^{\pi_{\mathrm{old}}}\left(\mathbf{s}_t\right)$ is for normalizing the probability distribution. It does not contribute to the gradient of the divergence with respect to policy parameters. 

This soft policy iteration algorithm, policy improvement and evaluation, is guaranteed to converge to the optimal maximum entropy policy under finite action space assumption and restricted policy domain $\Pi$. The convergence proofs are skipped for the sake of space, can be seen in \cite{HaarnojaAbbeelLevine2018:SAC}. 

\textbf{SAC Algorithm:} Previous sections were crucial to understand Soft Actor-Critic algorithm. For extending the learning process to continuous domains, authors leveraged neural networks as policy and state value function approximators \cite{HaarnojaAbbeelLevine2018:SAC}. In first version of the paper \cite{HaarnojaAbbeelLevine2018:SAC} they conceived a parameterized soft Q function $Q_{\theta}(s_t,a_t)$, value function $V_\psi(s_t)$, and a tractable policy $\pi_{\phi}(a_t \mid s_t)$, and target networks for value and critic networks $\bar{\psi}, \bar{\theta}$. The soft value function is updated through mean square error loss on sampled transactions from replay buffer $\mathcal{D}$: $J_V(\psi)=\mathbb{E}_{\mathbf{s}_t \sim \mathcal{D}}\left[\frac{1}{2}\left(V_\psi\left(\mathbf{s}_t\right)-\mathbb{E}_{\mathbf{a}_t \sim \pi_\phi}\left[Q_\theta\left(\mathbf{s}_t, \mathbf{a}_t\right)-\log \pi_\phi\left(\mathbf{a}_t \mid \mathbf{s}_t\right)\right]\right)^2\right].$ 
Parallel to that, the critic is updated through the state value approximated by target value network: $J_Q(\theta)=\mathbb{E}_{\left(\mathbf{s}_t, \mathbf{a}_t\right) \sim \mathcal{D}}\left[\frac{1}{2}\left(Q_\theta\left(\mathbf{s}_t, \mathbf{a}_t\right)- 
    r\left(\mathbf{s}_t, \mathbf{a}_t\right)-\gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim p}\left[V_{\bar{\psi}}\left(\mathbf{s}_{t+1}\right)\right]
    \right)^2\right].$ \label{critic_update:sac} The policy network is used to regress a Gaussian distribution and sample actions from it: $a_{t}=f_{\phi}(\epsilon_t; s_t)$ where $\epsilon_{t}$ is a noise sampled from univariate Normal distribution. The policy network is trained via reparameterization trick proposed in \cite{kingma2022autoencoding}. The update is done to minimize the expected KL divergence in equation \ref{eqn:soft_policy_kl}. Therefore, the gradient can be approximated with: $\nabla_\phi \hat{J}_\pi(\phi)=\nabla_\phi \alpha \log \pi_\phi\left(\mathbf{a}_t \mid \mathbf{s}_t\right)+\left(\nabla_{\mathbf{a}_t} \alpha \log \pi_\phi\left(\mathbf{a}_t \mid \mathbf{s}_t\right)-\nabla_{\mathbf{a}_t} Q\left(\mathbf{s}_t, \mathbf{a}_t\right)\right) \nabla_\phi\left(\boldsymbol{\epsilon}_t ; \mathbf{s}_t\right)$. The target networks are updated through soft updates. The next parts are for different variants of SAC algorithm.

\textbf{Auto-Tuning Temperature Hyperparameter $\alpha$:} The temperature hyper-parameter $\alpha$ is introduced to balance between exploration and exploitation \cite{HaarnojaAbbeelLevine2018:SAC}. This introduction was adding another hyper-parameter to tune across different environments. Additionally, this was also harming agent's scaling to different reward functions across different environments. In their follow-up work, they introduce a mechanism to update the temperature parameter $\alpha$ \cite{haarnoja2019soft}. This formula is obtained through solving the dual problem of maximizing entropy objective, 
Update formula and its proof is skipped due to space constraints. We implemented this update mechanism and observed its effects in section \ref{sac_results}.

\textbf{Modifying Value Network:} In the first version of the paper \cite{HaarnojaAbbeelLevine2018:SAC} authors employed a value and a critic network to be used in updating the actor. However it has been demonstrated that single critic network is susceptible to overestimation of the Q values \cite{lillicrap16ddpg}. This issue could be relieved with having two critic networks, minimum of which is taken for actor update \cite{fujimoto2018:TD3}. Two critic version takes place in a follow-up work \cite{haarnoja2019soft}. We also implemented the version with two critic and observed the results. 

\textbf{SR-SAC:} Recently it has been shown that the sample efficiency of the RL algorithms could be improved with increasing replay ratio under carefully designed conditions \cite{d'oro2023sampleefficient}. They have shown the effectiveness of resetting actor and critic networks in relatively high replay ratio settings. The motivation behind resetting is to eliminate the primacy bias, that is the agent's tendency to be affected by the formerly collected data \cite{nikishin2022primacy}. 

\subsection{Experimental evaluation \& Discussion}
\label{sac_results}

This section is for the experiments in above-mentioned settings. Hyper-parameters for SAC agent is tuned through manual search. For the best agent we have a learning rate of $3*10^{-4}$, buffer size of 300K, two hidden dimensions containing 256 hidden neurons. All settings except \textbf{Setting6}(S6) uses two critics for value estimation.

Upper part of Figure \ref{fig:perf_plot_sac} displays the performances of different SAC agents against weak and strong agents. \textbf{S0} is for default state and augmented reward, 
\textbf{S1} is for augmented state and  reward, \textbf{S2} is for default state and reward, \textbf{S3} is for augmented state and default reward. Among those setting we see that both augmentations are beneficial for learning. For the remaining settings we continue with augmented state and reward. \textbf{S4} is for Auto-Tuning $\alpha$, \textbf{S5} is for fixed $\alpha=0.1$, \textbf{S6} is for the vanilla version using a value and a critic network, \textbf{S7} is for scaling by reset version. We see that there is an advantage to using two critics instead of vanilla version. This is because of more accurate value estimation. Algorithm best performs with lower $\alpha$ values.  

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gfx/perf_plot_sac.pdf}
    \includegraphics[width=\linewidth]{gfx/sr_plot_sac.pdf}
    \caption{Upper: Performance of SAC agent trained in different settings against weak and strong agent, Lower: Effect of scaling by resetting in different settings against weak and strong agent}
    \label{fig:perf_plot_sac}
\end{figure}

Lower part of Figure  \ref{fig:perf_plot_sac} displays the effect of scaling by reset across different replay ratios. For implementation of SR-SAC the inverval of resetting is every 0.5M gradient steps of the networks. \textbf{S0} is for scaling with replay ratio of 1, \textbf{S1} is for scaling with replay ratio of 2, \textbf{S2} is for scaling with replay ratio of 4, \textbf{S3} is default version with replay ratio 1, \textbf{S4} is default version with replay ratio 2, \textbf{S5} is default version with replay ratio 4. We see that reset by scaling deteriorates performance. This may be the problem caused by the not finding an optimal interval value for resetting. On the other hand, for the default version it is beneficial to increase the replay ratio.

 


 
